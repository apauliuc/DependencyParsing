{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word and POS tags embeddings\n",
    "\n",
    "\n",
    "## Description\n",
    "In this notebook, we are trying to parse $*.conllu$ files and extract the word embeddings, as well as the POS tags emeddings.\n",
    "\n",
    "## Work to be done:\n",
    "1. Learn how to parse $*.conllu$ files\n",
    "2. Find a way to get all words embeddings.\n",
    "3. Find a way to get all POS tags embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read $*.conllu$ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = \"Annotated training data/UD_English-master/\"\n",
    "en_dev_filename = \"en-ud-dev.conllu\"\n",
    "en_test_filename = \"en-ud-test.conllu\"\n",
    "en_train_filename = \"en-ud-train.conllu\"\n",
    "\n",
    "ro_path = \"Annotated training data/UD_Romanian-dev/\"\n",
    "ro_dev_filename = \"ro-ud-dev.conllu\"\n",
    "ro_test_filename = \"ro-ud-test.conllu\"\n",
    "ro_train_filename = \"ro-ud-train.conllu\"\n",
    "\n",
    "def read_sentences(path, filename):\n",
    "    sentences = []\n",
    "    sentence_lines = []\n",
    "    file = open(path + filename, 'r')\n",
    "    for line in file:\n",
    "        if line == '\\n':\n",
    "            sentences.append(Sentence.from_lines(sentence_lines))\n",
    "            sentence_lines = []\n",
    "        else:\n",
    "            sentence_lines.append(line)\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "\n",
    "en_train_sentences = read_sentences(en_path,en_train_filename)\n",
    "en_dev_sentences = read_sentences(en_path,en_dev_filename)\n",
    "en_test_sentences = read_sentences(en_path,en_test_filename)\n",
    "\n",
    "ro_train_sentences = read_sentences(ro_path,ro_train_filename)\n",
    "ro_dev_sentences = read_sentences(ro_path,ro_dev_filename)\n",
    "ro_test_sentences = read_sentences(ro_path,ro_test_filename)\n",
    "\n",
    "assert len(en_dev_sentences) == 2002# there are 2002 sentences in the english development dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    \"\"\"Word class that maps all details for a word from a *.conllu file. \"\"\"\n",
    "    \n",
    "    def __init__(self, ID, FORM, LEMMA, UPOSTAG, XPOSTAG, FEATS, HEAD, DEPREL, DEPS, MISC):\n",
    "        self.ID = ID           # Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes.\n",
    "        self.FORM = FORM       # Word form or punctuation symbol.\n",
    "        self.LEMMA  = LEMMA    # Lemma or stem of word form.\n",
    "        self.UPOSTAG = UPOSTAG # Universal part-of-speech tag.\n",
    "        self.XPOSTAG = XPOSTAG # Language-specific part-of-speech tag; underscore if not available.\n",
    "        self.FEATS = FEATS     # List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "        self.HEAD = HEAD       # Head of the current word, which is either a value of ID or zero (0).\n",
    "        self.DEPREL = DEPREL   # Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "        self.DEPS = DEPS       # Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "        self.MISC = MISC       # Any other annotation.\n",
    "    \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        \n",
    "        string += self.ID      if self.ID      != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.FORM    if self.FORM    != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.LEMMA   if self.LEMMA   != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.UPOSTAG if self.UPOSTAG != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.XPOSTAG if self.XPOSTAG != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.FEATS   if self.FEATS   != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.HEAD    if self.HEAD    != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.DEPREL  if self.DEPREL  != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.DEPS    if self.DEPS    != None else '_'\n",
    "        string += \" \"\n",
    "        string += self.MISC    if self.MISC    != None else '_'\n",
    "        \n",
    "        return string\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_line(line):\n",
    "        tokens = line.split()\n",
    "        \n",
    "        assert len(tokens) == 10\n",
    "        \n",
    "        ID =      tokens[0] if tokens[0] != '_' else None\n",
    "        FORM =    tokens[1] if tokens[1] != '_' else None\n",
    "        LEMMA =   tokens[2] if tokens[2] != '_' else None\n",
    "        UPOSTAG = tokens[3] if tokens[3] != '_' else None\n",
    "        XPOSTAG = tokens[4] if tokens[4] != '_' else None\n",
    "        FEATS =   tokens[5] if tokens[5] != '_' else None\n",
    "        HEAD =    tokens[6] if tokens[6] != '_' else None\n",
    "        DEPREL=   tokens[7] if tokens[7] != '_' else None\n",
    "        DEPS =    tokens[8] if tokens[8] != '_' else None\n",
    "        MISC =    tokens[9] if tokens[9] != '_' else None\n",
    "        \n",
    "        return Word(ID, FORM, LEMMA, UPOSTAG, XPOSTAG, FEATS, HEAD, DEPREL, DEPS, MISC)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Word class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 proves prove VERB VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 0 root 0:root _\n",
      "proves\n"
     ]
    }
   ],
   "source": [
    "test_text = \"4\tproves\tprove\tVERB\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\"\n",
    "test_word = Word.from_line(text)\n",
    "print(test_word)\n",
    "print(test_word.FORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    \"\"\"Sentence class that maps all details for a sentence from a *.conllu file. \"\"\"\n",
    "    \n",
    "    def __init__(self,newdoc_id, send_id, text, words):\n",
    "        self.newdoc_id = newdoc_id\n",
    "        self.send_id = send_id\n",
    "        self.text = text\n",
    "        self.words = words\n",
    "        \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        if self.newdoc_id != None:\n",
    "            string += \"# newdoc id = \"\n",
    "            string += str(self.newdoc_id)\n",
    "            string += \"\\n\"\n",
    "        string += \"# send_id = \"\n",
    "        string += str(self.send_id)\n",
    "        string += \"\\n\"\n",
    "        string += \"# text = \"\n",
    "        string += str(self.text)\n",
    "        string += \"\\n\"\n",
    "        sentence_length = len(self.words)\n",
    "        for index, word in enumerate(self.words):\n",
    "            string += str(word)\n",
    "            if index < sentence_length - 1:\n",
    "                string += \"\\n\"\n",
    "        return string\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def from_lines(lines):\n",
    "        newdoc_id=''\n",
    "        send_id = ''\n",
    "        text=''\n",
    "        words = []\n",
    "\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith(\"#\"): #misc properties\n",
    "                prop = line.split(\"=\")[1]\n",
    "                if line.startswith(\"newdoc\", 2):\n",
    "                    newdoc_id = prop\n",
    "                    continue\n",
    "                if line.startswith(\"sent_id\", 2):\n",
    "                    send_id = prop\n",
    "                    continue\n",
    "                if line.startswith(\"text\", 2):\n",
    "                    text = prop\n",
    "                    continue\n",
    "                continue\n",
    "            else:   # words\n",
    "                words.append(Word.from_line(line))\n",
    "\n",
    "        return Sentence(newdoc_id, send_id, text, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Sentence class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = \n",
      "# send_id =  weblog-blogspot.com_gettingpolitical_20030906235000_ENG_20030906_235000-0003\n",
      "# text =  Today's incident proves that Sharon has lost his patience and his hope in peace.\n",
      "1 Today today NOUN NN Number=Sing 3 nmod:poss 3:nmod:poss SpaceAfter=No\n",
      "2 's 's PART POS _ 1 case 1:case _\n",
      "3 incident incident NOUN NN Number=Sing 4 nsubj 4:nsubj _\n",
      "4 proves prove VERB VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 0 root 0:root _\n",
      "5 that that SCONJ IN _ 8 mark 8:mark _\n",
      "6 Sharon Sharon PROPN NNP Number=Sing 8 nsubj 8:nsubj _\n",
      "7 has have AUX VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 8 aux 8:aux _\n",
      "8 lost lose VERB VBN Tense=Past|VerbForm=Part 4 ccomp 4:ccomp _\n",
      "9 his he PRON PRP$ Gender=Masc|Number=Sing|Person=3|Poss=Yes|PronType=Prs 10 nmod:poss 10:nmod:poss _\n",
      "10 patience patience NOUN NN Number=Sing 8 obj 8:obj _\n",
      "11 and and CCONJ CC _ 13 cc 13:cc _\n",
      "12 his he PRON PRP$ Gender=Masc|Number=Sing|Person=3|Poss=Yes|PronType=Prs 13 nmod:poss 13:nmod:poss _\n",
      "13 hope hope NOUN NN Number=Sing 10 conj 10:conj _\n",
      "14 in in ADP IN _ 15 case 15:case _\n",
      "15 peace peace NOUN NN Number=Sing 13 nmod 13:nmod SpaceAfter=No\n",
      "16 . . PUNCT . _ 4 punct 4:punct _ \n",
      "\n",
      " Today's incident proves that Sharon has lost his patience and his hope in peace. \n",
      "\n",
      "1 Today today NOUN NN Number=Sing 3 nmod:poss 3:nmod:poss SpaceAfter=No \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\"# sent_id = weblog-blogspot.com_gettingpolitical_20030906235000_ENG_20030906_235000-0003\",\n",
    "\"# text = Today's incident proves that Sharon has lost his patience and his hope in peace.\",\n",
    "\"1\tToday\ttoday\tNOUN\tNN\tNumber=Sing\t3\tnmod:poss\t3:nmod:poss\tSpaceAfter=No\",\n",
    "\"2\t's\t's\tPART\tPOS\t_\t1\tcase\t1:case\t_\",\n",
    "\"3\tincident\tincident\tNOUN\tNN\tNumber=Sing\t4\tnsubj\t4:nsubj\t_\",\n",
    "\"4\tproves\tprove\tVERB\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\",\n",
    "\"5\tthat\tthat\tSCONJ\tIN\t_\t8\tmark\t8:mark\t_\",\n",
    "\"6\tSharon\tSharon\tPROPN\tNNP\tNumber=Sing\t8\tnsubj\t8:nsubj\t_\",\n",
    "\"7\thas\thave\tAUX\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t8\taux\t8:aux\t_\",\n",
    "\"8\tlost\tlose\tVERB\tVBN\tTense=Past|VerbForm=Part\t4\tccomp\t4:ccomp\t_\",\n",
    "\"9\this\the\tPRON\tPRP$\tGender=Masc|Number=Sing|Person=3|Poss=Yes|PronType=Prs\t10\tnmod:poss\t10:nmod:poss\t_\",\n",
    "\"10\tpatience\tpatience\tNOUN\tNN\tNumber=Sing\t8\tobj\t8:obj\t_\",\n",
    "\"11\tand\tand\tCCONJ\tCC\t_\t13\tcc\t13:cc\t_\",\n",
    "\"12\this\the\tPRON\tPRP$\tGender=Masc|Number=Sing|Person=3|Poss=Yes|PronType=Prs\t13\tnmod:poss\t13:nmod:poss\t_\",\n",
    "\"13\thope\thope\tNOUN\tNN\tNumber=Sing\t10\tconj\t10:conj\t_\",\n",
    "\"14\tin\tin\tADP\tIN\t_\t15\tcase\t15:case\t_\",\n",
    "\"15\tpeace\tpeace\tNOUN\tNN\tNumber=Sing\t13\tnmod\t13:nmod\tSpaceAfter=No\",\n",
    "\"16\t.\t.\tPUNCT\t.\t_\t4\tpunct\t4:punct\t_\"]\n",
    "sentence = Sentence.from_lines(lines)\n",
    "print(str(sentence), \"\\n\")\n",
    "print(sentence.text, \"\\n\")\n",
    "print(sentence.words[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract words/POS tags and then extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: extract words/POS tags and then extract embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
